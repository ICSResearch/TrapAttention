<div align="center">

# **Trap Attention**  
### *Monocular Depth Estimation with Manual Traps â€” Implementation*  
*(Clean, centered layout for clarity and style.)*

</div>

---

## ğŸ“‘ Table of Contents  
1. [Paper & Poster](#paper--poster)  
2. [Supplemental Materials](#supplemental-materials)  
3. [Code & Checkpoint](#code--checkpoint)  
4. [Environment](#environment)  
5. [Usage](#usage)  
6. [Citation & If This Helps, Please Cite](#citation--if-this-helps-please-cite)  
7. [Acknowledgement](#acknowledgement)  
8. [Other / TODO](#other--todo)  

---

## ğŸ“° Paper & Poster

**Paper:**  
[Trap Attention: Monocular Depth Estimation With Manual Traps â€” CVPR 2023](https://openaccess.thecvf.com/content/CVPR2023/html/Ning_Trap_Attention_Monocular_Depth_Estimation_With_Manual_Traps_CVPR_2023_paper.html) :contentReference[oaicite:0]{index=0}  

**Poster & Supplement & Related Materials:**  
- Poster / Virtual Session: available via CVPR 2023 Poster Page :contentReference[oaicite:1]{index=1}  
- Supplemental PDF (network details / extra results): accessible here â€“ [Supplemental File](https://openaccess.thecvf.com/content/CVPR2023/supplemental/Ning_Trap_Attention_Monocular_CVPR_2023_supplemental.pdf) :contentReference[oaicite:2]{index=2}  

*(If you want, you can download the poster and place under `./docs/` or `./posters/` and insert display.)*

---

## ğŸ“¦ Code & Checkpoint

- Implementation repository: [ICSResearch/TrapAttention on GitHub](https://github.com/ICSResearch/TrapAttention) :contentReference[oaicite:3]{index=3}  
- Pretrained / checkpoint models: (as you provided earlier) Google Drive link.

```text
Google Drive: https://drive.google.com/drive/folders/1kIXg9UP0cVWUq_7Pq20JT9_RyR-PjvkS?usp=sharing
ğŸ§° Environmentâ€¢ Python 3.8â€¢ PyTorch 1.7.1 (or greater, as long as compatible)
(You may install other dependencies per requirements.txt in the repo.)â–¶ï¸ Usage / Quick StartClone the repo, download the checkpoint, and run as follows (for example):bashå¤åˆ¶ä»£ç git clone https://github.com/ICSResearch/TrapAttention.git
cd TrapAttention
```
# install dependencies
pip install -r requirements.txt

# example usage
python your_run_script.py --config configs/your_config.yaml  # adjust as needed
ğŸ“ Tip: Make sure CUDA / GPU memory is sufficient if you run highâ€‘res inputs or large batch size.ğŸ“š Citation â€“ If This Code Helps, Please CiteIf you use this code (or parts of it) in your work, please cite:bibtexå¤åˆ¶ä»£ç @InProceedings{Ning_2023_CVPR,
  author    = {Chao Ning and Hongping Gan},
  title     = {Trap Attention: Monocular Depth Estimation With Manual Traps},
  booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision and Pattern Recognition (CVPR)},
  year      = {2023},
  pages     = {5033â€“5043}
}

ğŸ™ AcknowledgementThanks to the following outstanding works / libraries / communities:â€¢ Transformers / Visionâ€‘Transformer backbones used in encoderâ€¢ The community maintaining openâ€‘source depthâ€‘estimation toolboxesâ€¢ All contributors and testers who reported bugs or improvementsâš ï¸ Other / TODOâ€¢ (Optional) Add inference examples & sample outputs in ./examples/â€¢ (Optional) Add visualization of depth maps (RGB â†’ depth) in README / docsâ€¢ (Optional) Add evaluation scripts and result tables
